"""
AI æœåŠ¡æ¨¡å—
è§†è§‰å®šä½å’Œå›¾åƒè¯†åˆ«ï¼ˆå½“å‰ä¸º Mock å®ç°ï¼‰
"""
import random
from typing import List, Optional
from app.schemas import LocationCandidate
from app.services.graph_service import graph_service
from app.core.config import settings


class AIService:
    """
    AI æœåŠ¡ç±»
    æä¾›å›¾åƒè¯†åˆ«å’Œè§†è§‰å®šä½åŠŸèƒ½
    
    å½“å‰å®ç°ï¼šMock æ•°æ®
    æœªæ¥å®ç°ï¼šCLIP æ¨¡å‹ + FAISS å‘é‡æ£€ç´¢
    """
    
    def __init__(self):
        self._model_loaded = False
        self._mock_mode = settings.AI_MOCK_MODE
    
    async def load_model(self) -> None:
        """
        åŠ è½½ AI æ¨¡å‹
        
        TODO: å®ç° CLIP æ¨¡å‹åŠ è½½
        - ä½¿ç”¨ sentence-transformers æˆ– transformers åº“
        - åŠ è½½é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹
        - åˆå§‹åŒ– FAISS ç´¢å¼•
        """
        if self._mock_mode:
            # Mock æ¨¡å¼ä¸‹ä¸éœ€è¦åŠ è½½æ¨¡å‹
            self._model_loaded = True
            return
        
        # TODO: å®é™…æ¨¡å‹åŠ è½½ä»£ç 
        # from sentence_transformers import SentenceTransformer
        # self.model = SentenceTransformer(settings.CLIP_MODEL_NAME)
        # self._model_loaded = True
        pass
    
    async def recognize_location(
        self,
        image_data: bytes,
        top_k: int = 3
    ) -> List[LocationCandidate]:
        """
        è¯†åˆ«å›¾ç‰‡ä¸­çš„ä½ç½®
        
        Args:
            image_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            top_k: è¿”å›å‰ K ä¸ªå€™é€‰ç»“æœ
            
        Returns:
            LocationCandidate åˆ—è¡¨ï¼ŒæŒ‰ç½®ä¿¡åº¦é™åºæ’åˆ—
            
        TODO: å®ç°æ­¥éª¤
        1. OCR è¯†åˆ«ï¼ˆå¯é€‰ï¼‰ï¼šå°è¯•è¯†åˆ«å›¾ä¸­æ–‡å­—
        2. CLIP ç¼–ç ï¼šå°†å›¾ç‰‡è½¬ä¸ºå‘é‡
        3. å‘é‡æ£€ç´¢ï¼šä¸é¢„å­˜çš„èŠ‚ç‚¹å‘é‡åº“æ¯”å¯¹
        4. è¿”å› Top K ç»“æœ
        """
        if self._mock_mode:
            return await self._mock_recognize(top_k)
        
        # TODO: å®é™…è¯†åˆ«ä»£ç 
        # 1. OCR è¯†åˆ«
        # ocr_result = await self._ocr_recognize(image_data)
        # if ocr_result:
        #     return ocr_result
        
        # 2. CLIP å‘é‡æ£€ç´¢
        # image_vector = await self._extract_image_features(image_data)
        # candidates = await self._search_similar_nodes(image_vector, top_k)
        # return candidates
        
        return await self._mock_recognize(top_k)
    
    async def _mock_recognize(self, top_k: int = 3) -> List[LocationCandidate]:
        """
        Mock è¯†åˆ«å®ç°
        éšæœºè¿”å›ä¸€äº›èŠ‚ç‚¹ä½œä¸ºå€™é€‰ç»“æœ
        """
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"ğŸ² [Mockè¯†åˆ«] å¼€å§‹ Mock è¯†åˆ«ï¼Œè¯·æ±‚ top_k={top_k}")
        
        all_nodes = graph_service.get_all_nodes()
        logger.info(f"ğŸ“Š [Mockè¯†åˆ«] å›¾ä¸­å…±æœ‰ {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
        
        if not all_nodes:
            logger.warning("âš ï¸ [Mockè¯†åˆ«] å›¾ç»“æ„ä¸­æ²¡æœ‰èŠ‚ç‚¹æ•°æ®")
            return []
        
        # éšæœºé€‰æ‹©èŠ‚ç‚¹
        sample_size = min(top_k, len(all_nodes))
        selected_nodes = random.sample(all_nodes, sample_size)
        logger.info(f"ğŸ¯ [Mockè¯†åˆ«] éšæœºé€‰æ‹©äº† {sample_size} ä¸ªèŠ‚ç‚¹")
        
        # ç”Ÿæˆéšæœºç½®ä¿¡åº¦ï¼ˆæŒ‰é™åºæ’åˆ—ï¼‰
        confidences = sorted([random.uniform(0.5, 0.95) for _ in range(sample_size)], reverse=True)
        
        candidates = []
        for i, node in enumerate(selected_nodes):
            candidate = LocationCandidate(
                node_id=node["id"],
                node_name=node["name"],
                detail=node.get("detail"),
                floor=node["floor"],
                confidence=round(confidences[i], 2),
            )
            candidates.append(candidate)
            logger.info(f"  [{i+1}] èŠ‚ç‚¹: {node['name']} (ID: {node['id']}, æ¥¼å±‚: {node['floor']}, ç½®ä¿¡åº¦: {candidate.confidence})")
        
        logger.info(f"âœ… [Mockè¯†åˆ«] å®Œæˆï¼Œè¿”å› {len(candidates)} ä¸ªå€™é€‰ç»“æœ")
        return candidates
    
    async def _extract_image_features(self, image_data: bytes):
        """
        æå–å›¾åƒç‰¹å¾å‘é‡
        
        TODO: ä½¿ç”¨ CLIP æ¨¡å‹æå–ç‰¹å¾
        
        Args:
            image_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            
        Returns:
            ç‰¹å¾å‘é‡ (numpy array)
        """
        # TODO: å®ç°å›¾åƒç‰¹å¾æå–
        # from PIL import Image
        # import io
        # image = Image.open(io.BytesIO(image_data))
        # features = self.model.encode(image)
        # return features
        pass
    
    async def _search_similar_nodes(
        self,
        query_vector,
        top_k: int = 3
    ) -> List[LocationCandidate]:
        """
        åœ¨å‘é‡åº“ä¸­æœç´¢ç›¸ä¼¼èŠ‚ç‚¹
        
        TODO: ä½¿ç”¨ FAISS æˆ–ç®€å•ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›å‰ K ä¸ªç»“æœ
            
        Returns:
            LocationCandidate åˆ—è¡¨
        """
        # TODO: å®ç°å‘é‡æ£€ç´¢
        # distances, indices = self.faiss_index.search(query_vector, top_k)
        # æˆ–ä½¿ç”¨ç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦
        # similarities = cosine_similarity(query_vector, node_vectors)
        pass
    
    async def _ocr_recognize(self, image_data: bytes) -> Optional[List[LocationCandidate]]:
        """
        OCR è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
        
        TODO: ä½¿ç”¨ PaddleOCR è¯†åˆ«æ–‡å­—
        
        Args:
            image_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            
        Returns:
            å¦‚æœè¯†åˆ«åˆ°åŒ¹é…çš„èŠ‚ç‚¹ï¼Œè¿”å› LocationCandidate åˆ—è¡¨
            å¦åˆ™è¿”å› None
        """
        # TODO: å®ç° OCR è¯†åˆ«
        # from paddleocr import PaddleOCR
        # ocr = PaddleOCR()
        # result = ocr.ocr(image_data)
        # è§£æç»“æœï¼ŒåŒ¹é…èŠ‚ç‚¹åç§°
        pass


# å…¨å±€å•ä¾‹
ai_service = AIService()

